Dataset Descriptions for Citation Behavior project

In this document all of the relevant datasets used for and created for the
citation behavior project are outlined. These are almost all derivatives of
the thomson-reuters web of science

----------------------------------------------------------------------------

CONTENTS

1. Web Of Science

2. Jevin West Edge Database

	a. wos.net file
	b. sqlite database
	c. mapping.pkl
	d. edges

3. Intermediates of WoS

	a. "Acceptable" papers
	b. qualtrics-formatted csvs
	c. vis.html
	d. out_field.json
	e. out_percentile.json

----------------------------------------------------------------------------

1. Thomson-Reuters Web Of Science

The web of science (WoS) is where almost all of the data we use is from
in some capacity. It is a database of scientific documents, journals, and 
conferences going back to 1960. There are over 1 billion citations in WoS
and 57136685 publications. There is a site-specific version of the web of
science hosted at the knowledge lab, in Cloud Kotta, in a MySQL database 
on an Amazon AWS RDS server. 

A detailed schema is available here:
	http://docs.cloudkotta.org/dataguide/wos.html

There are two databases in the Klab WoS: wos and wos2. wos and wos2 are 
mostly the same (have almost the same schema) but wos is broken up into
one table per year (there is a 2015_publications rather than publications)
also, abstracts are in a separate table in wos2 but are in publications in
wos

The database is indexed on the field wos_id, which is a unique identifier
assigned to each paper/peice of material in the database. Each WoS id is
unique and because it is per paper, a unique WoS id does not imply a 
unique author.

There is also an attempt at a unique identifier for people, which was created
using topic modeling and clustering from there. This is in the field 
cluster_id, in the contributors table. It is not perfect and there are
instances of single people with multiple cluster_id s as well as multiple
people, usually with the same name with a single cluster_id

The most commonly used tables are publications, contributors, and refrences

More detailed documentation of what is in the WoS is availiable here:
	http://ip-science.thomsonreuters.com/m/pdfs/mgr/ws-wos-8-0-0807.pdf

WoS is considered sensitive and cannot leave the secure cloudKotta enclave

----------------------------------------------------------------------------

2. Jevin West edge database

At some point, the Klab acquired data from Jevin West representing a graph of
the citations among the WoS. This graph was provided in a space-effiecient 
format that is somewhat difficult to use. Because of that, a small SQLite 
database was set up to make it easier to work with

Because the only proprietary data in this dataset is wos_ids, which do not
contain any curated information or tell anyone anything about papers in 
the WoS, this dataset is safe to use outside of the enclave.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	a. wos.net file

	The file was  provided as a .net file, which is a two part space
	separated relational file, in one file. The header of each section is 
	*SECTION TITLE <NUMBER OF ITEMS> and there are two sections: vertecies
	and edges. Vertecies is a mapping of wos_ids to integers, which are 
	more efficient than strings, and edges is a source target network of
	edges A very small example of this is below:
		*vertices 8
		0 "WOS:000187518000016"
		1 "WOS:000187518000016.1"
		2 "WOS:000202176100004.3"
		3 "WOS:000187518000016.3"
		4 "WOS:000200960600144"
		5 "WOS:000202637300042.12"
		6 "WOS:000200960600144.2"
		7 "WOS:000200960600144.1"
		8 "WOS:000204039400025"
		*edges 5
		0 1
		0 3
		1 2
		4 8
		5 6

	#Note: Those links are not real and were made up to show format.

	The wos.net contains 219963473 vertices and 1034566885 edges. The file
	is 26GB of plaintext and is pretty difficult to work directly out of.
	For example, the command "cat wos.net >> /dev/null" (i'm using this as
	a measure of best case I/O overhead) takes 3M:31S

	Because it is so difficult to work with this file, a SQLite database was
	made so that citation counts and subnetworks coule be easily extracted 
	from the file.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	b. wos SQLite DB.

	SQLite is a file-based SQL compliant relational database system which 
	was used to store the contents of the wos.net file.

	It has two tables: 'mapping' and 'edges'. 

	'mapping' contains two fields: INT int_id and TEXT wos_id. 
	int_id is a primary key and both fields are unique. 
	They represent the integer id in the wos.net and the wos_id.

	edges also contains two fields, both integers: source and target. They
	both correspond directly to wos_ids in the mapping. The edges table 
	represents the wos.net file completely. There are scripts designed to 
	upload  points into the database which are outlined in the file 
	file_descriptions.txt

	The file is 32 GB and accessible from within kotta at 
	s3://klab-webofscience/citation_network/edge_data.db

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	c. mapping.pkl

	This file was created because the mapping section of the wos.net
	plaintext file was simply too large to hold in memory at once and
	python couldn't handle it.

	The structure is a python pickle file containing serialized objects
	in the format of {STRING wos_id: INT int_id}. There is one object per
	paper in the wos.net file. The mapping.pkl file itself is 9.2gb.

	Uploading the entire file to the sqlite database took about 18 hours
	of computation so caution is advised before trying to do that again

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	d. edges

	This file is just the edges section of the wos.net file. It was used for
	uploading the wos.net file into the database. It's structure is exactly
	the same as the edges section is as documented above. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

----------------------------------------------------------------------------

3. Intermediates of WoS

	Because the end product of this stage of this project is a qualtrics 
	survey, we need to keep in mind the format of data which qualtrics 
	needs.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	a. Lists of "Acceptable" papers

	These are python string representations of 2 dimensional tuples
	containing the sampling frame for our survey. More details are in
	sampling_strategy.txt, but essentially, these are papers for a 
	specific field such that we have the email of the first author.

	Here is a truncated example from wos_ids_Surgery.txt: 

		(('WOS:000360383600010',), ('WOS:000351190500017',),
		 ('WOS:000359689200001',), ('WOS:000350752500025',), 
		 ('WOS:000349803400003',), ('WOS:000349515400002',), 
		 ('WOS:000358693900016',), ('WOS:000356320400023',), 
		 ('WOS:000352199800020',), ('WOS:000353962700021',), 
		 ('WOS:000357189000004',), ('WOS:000358375200012',), 
		 ('WOS:000360940500453',), ('WOS:000351015300001',), 
		 ('WOS:000356179000006',), ('WOS:000359689200001',), 
		 ('WOS:000359689200001',), ('WOS:000349803400003',), 
		 ('WOS:000357946400013',), ('WOS:000354035500033',), 
		 ('WOS:000358782100024',), ('WOS:000347152300006',), 
		 ('WOS:000347684100004',), ('WOS:000354723200020',), 
		 ('WOS:000351061900035',), ('WOS:000352780800004',), 
		 ('WOS:000346489701375',))

	It is possible to read these files into a program with the python 
	module ast (abstract syntax tree) using something like this:

	papers_list = ast.literal_eval(open("wos_ids_{}.txt".format(field)).read())

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	b. qualtrics-formatted csvs

	We are asking people about things they have done, which requires us to 
	know what they have done and format our survey such that we can show 
	them past papers they have written. Qualtrics allows this through 
	embedded data fields, which can be submitted as csv files in a 
	specific format. For our survey, the following is an example header row
	of a qualtrics survey.

	First Name,Last Name,Email,title,abstract,year,amzn,title1,journal1,authors1,abstract1,year1,title2,journal2,authors2,abstract2,year2,title3,journal3,authors3,abstract3,year3,citecount

	Almost all of these fields are self-explainatory but a few are not:
		amzn: amazon gift code

		abstract,journal,title,year WITHOUT NUMBER: attributes of
		the author's paper

		same as above but with 1,2, or 3: attributes of papers that we 
		are asking about

		citecount: number of times that a source has been cited. Which
		source this is is randomly selected	

	More description about why certain things were chosen is available in 
	question_info.txt

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	c. vis.html

	This is a vis.js graphical visualization of what we have in the WoS,
	Broken down by field and showing counts and percentages with emails
	availiable. This was generated with graphs.py and was/will be useful
	for deciding what fields we want to survey. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	d. out_field.json

	out_field.json is a python dictionary/json file which contains
	information about what is in the web of science. It was used to 
	generate the graph in vis.html. It contains counts of papers,
	percentage of those papers with email addresses attached to them,
	and correlations of percentiles (see sec. 3 e for more information)
	per field. Example format is below:

	 {
       {"Field":[
           count, percentage, correlation
      ]},
      {"Field2":[
          count, percentage, correlation
      ]}
     } 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	e. out_percentile.json

	Contains percentile breakdowns of email coverage per percentile by
	citation count. Authors (using cluster_id) were broken up into 100
	sections of equal size based on the number of citations there papers
	have, and the percentage of papers with emails was computed for each
	bin. Example format below:

	{
	    "1": 0.5444133367269025, 
	    "2": 0.5128531432934589, 
	    "3": 0.535505217612624, 
	    "4": 0.5189615678289641,
	    ...
	    "99": 0.5189615678289641,
	}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

----------------------------------------------------------------------------



